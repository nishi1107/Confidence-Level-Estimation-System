# -*- coding: utf-8 -*-
"""1-Confidence_Level_Estimation (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5ub1PDxW_TWTatZHvuHB-xOnFqJuIvE
"""

pip install deepface

pip install mediapipe

pip install face_recognition

pip install fpdf

# Commented out IPython magic to ensure Python compatibility.
import cv2
import datetime
import math
import sys
from deepface import DeepFace
import mediapipe as mp
import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')
import pickle
import face_recognition as fr
import numpy as np
from PIL import Image
import dlib
from fpdf import FPDF
import matplotlib.pyplot as plt
# %matplotlib inline
import collections

global eye_mov_count
eye_mov_count = 1

class ConfidenceEstimation:

    def load_model(self):
        """
        DESCRIPTION:
        Loads the final model to predict confidence

        RETURNS:
        Both the models
        """
        filename = "/content/drive/MyDrive/Project/Codes/Confidence_Classifier.sav"
        inp = [[4,0,0],
              [2,0,0]]
        load_model = pickle.load(open(filename, 'rb'))
        res = load_model.predict(inp)
        print(res)

    def get_video_duration(self, path):
        data = cv2.VideoCapture(path)
        frames = data.get(cv2.CAP_PROP_FRAME_COUNT)
        fps = data.get(cv2.CAP_PROP_FPS)
        seconds = round(frames/fps)
        video_time = datetime.timedelta(seconds=seconds)
        return video_time
    
    def VideoToFrame(self, path):
        """
        DESCRIPTION:
        Converts video to fixed number of frames

        RETURNS:
        Frames of uploaded video
        """

        data = cv2.VideoCapture(path)
        frames = data.get(cv2.CAP_PROP_FRAME_COUNT)
        fps = data.get(cv2.CAP_PROP_FPS)
        seconds = round(frames/fps)
        video_time = datetime.timedelta(seconds=seconds)
        interval = math.floor(seconds/250)
          
        count = 1
        success = 1
        sec = 0
        self.no_of_face_detected = 0
          
        while success and count <= 200:
            data.set(cv2.CAP_PROP_POS_MSEC, sec*1000)
            success, image=data.read() 
              
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
            faces = faceCascade.detectMultiScale(
                gray,
                scaleFactor = 1.3,
                minNeighbors = 3,
                minSize = (30, 30)
            )
              
            self.no_of_face_detected += len(faces)
            print(self.no_of_face_detected)
            for (x, y, w, h) in faces:
                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)
                roi_color = image[y:y+h, x:x+w]
                cv2.imwrite('f_'+str(count)+'.jpg', roi_color)
                count += 1
                sec += interval

        return self.no_of_face_detected



    def getFaceExpressions(self,path):
        """
        DESCRIPTION:
        Finds face expression from every frame

        RETURNS:
        Face expression values
        """
        count = 1
        exp = []
        while count <= 200:
            im = "f_" + str(count) + ".jpg"
            img_name = path + im
            img_path = img_name
            
            try:
                demography = DeepFace.analyze(img_path, actions=["emotion"])
                exp.append(demography[0]["dominant_emotion"])
                count += 1
            except ValueError:
                count += 1
        return exp




    
    def getFaceDirection(self,path):
        """
        DESCRIPTION:
        Finds face direction from every frame

        RETURNS:
        Face direction values
        """
        count=1
        dir = []
        while count<=200:
          try:
              im = "f_" + str(count) + ".jpg"
              img_name = path + im
              img_path = img_name
              

              mp_face_mesh = mp.solutions.face_mesh
              face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)
              mp_drawing = mp.solutions.drawing_utils
              drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

              img = cv2.imread(img_path)
              image = cv2.cvtColor(cv2.flip(img, 1), cv2.COLOR_BGR2RGB)
              image.flags.writeable = False
              results = face_mesh.process(image)
              #print(results)
              image.flags.writeable = True

              image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
              img_h, img_w, img_c = image.shape
              face_3d = []
              face_2d = []
              if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                  for idx, lm in enumerate(face_landmarks.landmark):
                    if idx==33 or idx==263 or idx==1 or idx==61 or idx==291 or idx==199:
                      if idx==1:
                        nose_2d = (lm.x*img_w, lm.y*img_h)
                        nose_3d = (lm.x*img_w, lm.y*img_h, lm.z*3000)
                      x,y = int(lm.x*img_w), int(lm.y * img_h)
                      face_2d.append([x,y])
                      face_3d.append([x, y, lm.z])
                  face_2d = np.array(face_2d, dtype=np.float64)
                  face_3d = np.array(face_3d, dtype=np.float64)
                  focal_length = 1*img_w
                  cam_matrix = np.array([[focal_length, 0, img_h/2],
                                        [0, focal_length, img_w/2],
                                        [0, 0, 1]])
                  dist_matrix = np.zeros((4,1), dtype=np.float64)
                  success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)
                  rmat, jac = cv2.Rodrigues(rot_vec)
                  angles, mtxR, ntxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)
                  x = angles[0]*360
                  y = angles[1]*360
                  z = angles[2]*360

                  if y < -10:
                    text = "Looking Left"
                  elif y > 10:
                    text = "Looking Right"
                  elif x < -10:
                    text = "Looking Down"
                  elif x > 10:
                    text = "Looking Up"
                  else:
                    text = "Forward"

                  dir.append(text)
                  count += 1
                  
          except BaseException:
                  count += 1
        return dir


    def pil2cv(self, img_array):
      """ Convert PIL image array to OpenCV image array
      :param img_array: PIL image object
      :return: OpenCV image object
      """
      return cv2.cvtColor(np.asarray(img_array), cv2.COLOR_RGB2BGR)

    def cv2pil(self, img_array):
      """ Convert OpenCV image array to PIL image array
      :param img_array: OpenCV image object
      :return: PIL image object
      """
      return Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))

    def cv2fr(self, img_array):
      """ Convert OpenCV image array to lib face_recognition image array
      :param img_array: OpenCV image object
      :return: face_recognition image object
      """
      return cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)

    def pil2fr(self, img_array):
      """ Convert PIL image array to lib face_recognition image array
      :param img_array: PIL image object
      :return: face_recognition image object
      """
      return np.array(img_array)

    def fr2pil(self, img_array):
      """ Convert lib face_recognition image array to PIL image array
      :param img_array: face_recognition image object
      :return: PIL image object
      """
      return Image.fromarray(np.uint8(img_array))

    def get_face_position(self, fr_img_array):
      """ Get the face location in the image
      :param fr_img_array: face_recognition image array
      :return: A list of tuples of found face locations in css order (top, right, bottom, left), like [(171, 409, 439, 141)]
      """
      self.face_location = fr.api.face_locations(self.fr_img_array)
      return self.face_location

    def get_eye_positions(self, fr_img_array, face_location):
      """
      :param fr_img_array: An image (as a numpy array)
      :param face_location: face location
      :return: dict include eyes locations
      """
      self.face_landmarks = fr.api.face_landmarks(self.fr_img_array, face_locations=self.face_location)
      if self.face_landmarks:
          self.left_eye = self.face_landmarks[0]["left_eye"]
          self.right_eye = self.face_landmarks[0]["right_eye"]
          self.eye_locations = {"left_eye": self.left_eye, "right_eye": self.right_eye}
          return self.eye_locations
      else:
          return None

    def image_preprocessing(self, cv_img_array):
      """ Image preprocessing. Adjust image size.
      :param cv_img_array: image array returned by cv2.imread
      :return: a dict containing OpenCV image array and the eye positions
      """
      if cv_img_array.shape[1] > 500:
          cv_img_array = cv2.resize(cv_img_array, (0, 0), fx=0.4, fy=0.4)
      self.fr_img_array = self.CV2FR(cv_img_array)
      self.face_location = self.get_face_position(self.fr_img_array)
      if self.face_location:
          cv_img_array = cv_img_array[self.face_location[0][0]:self.face_location[0][2], self.face_location[0][3]:self.face_location[0][1]]
          self.eye_locations = self.get_eye_positions(self.fr_img_array, self.face_location)

          for i in range(6):
              self.height = self.eye_locations["left_eye"][i][0] - self.face_location[0][3]
              self.width = self.eye_locations["left_eye"][i][1] - self.face_location[0][0]
              self.eye_locations["left_eye"][i] = (self.height, self.width)

          for i in range(6):
              self.height = self.eye_locations["right_eye"][i][0] - self.face_location[0][3]
              self.width = self.eye_locations["right_eye"][i][1] - self.face_location[0][0]
              self.eye_locations["right_eye"][i] = (self.height, self.width)

          self.result = {"cv_img_array": cv_img_array, "eye_locations": self.eye_locations}
          return self.result
      else:
          return None

    def get_pupil_position(self, width, height, center):
      """ Parse the pupil position
      :param width: the width of the eye socket
      :param height: the height of the eye socket
      :param center: the center position of the eye
      :return: the pupil position
      """
      self.width_trisection = int(self.width / 3)
      self.height_trisection = int(self.height / 3)

      if (center["x"] < self.width_trisection) & (center["y"] < self.height_trisection):
          return 0
      elif (center["x"] > self.width_trisection * 2) & (center["y"] < self.height_trisection):
          return 2
      elif center["y"] < self.height_trisection:
          return 1
      elif (center["x"] < self.width_trisection) & (center["y"] > self.height_trisection * 2):
          return 6
      elif (center["x"] > self.width_trisection * 2) & (center["y"] > self.height_trisection * 2):
          return 8
      elif center["y"] > self.height_trisection * 2:
          return 7
      elif center["x"] < self.width_trisection:
          return 3
      elif center["x"] > self.width_trisection * 2:
          return 5
      else:
          return 4


    def rect_eye(self,eye_landmarks):
      """ Get the rectangular box of the eye from the eye position produced by function face_recognition.face_landmarks
      :param eye_landmarks: the eye position, like [(193, 251), (205, 242), (223, 244), (235, 258), (220, 261), (203, 259)]
      :return: A dict containing the coordinate of the upper left corner (x1,y1) and the coordinate of the lower right corner (x2,y2)
      """
      self.width = eye_landmarks[3][0] - eye_landmarks[0][0]
      self.height = int(
          (eye_landmarks[4][1] + eye_landmarks[5][1] - eye_landmarks[1][1] - eye_landmarks[2][1]) / 2
      )
      self.x = eye_landmarks[0][0]
      self.y = int(
          (eye_landmarks[1][1] + eye_landmarks[2][1]) / 2
      )
      return {"x1": self.x, "y1": self.y, "x2": self.x + self.width, "y2": self.y + self.height}


    def gaze_direction(self, cv_img_array, eye_locations):
      """
      :param cv_img_array: OpenCV image array
      :param eye_locations: coordinates of eyes
      """
      cv_img_array = np.uint8(np.clip(1.1 * cv_img_array + 30, 0, 255))
      self.left_coordinate = self.rect_eye(self.eye_locations["left_eye"])
      self.right_coordinate = self.rect_eye(self.eye_locations["right_eye"])

      self.left_eyeball = self.get_eyeball_position(cv_img_array, self.left_coordinate)
      self.left_percent = self.left_eyeball["percent"]
      self.left_pupil_position = self.left_eyeball["pupil_position"]

      self.right_eyeball = self.get_eyeball_position(cv_img_array, self.right_coordinate)
      self.right_percent = self.right_eyeball["percent"]
      self.right_pupil_position = self.right_eyeball["pupil_position"]

      return [self.left_pupil_position, self.left_percent, self.right_pupil_position, self.right_percent]

    def get_eyeball_position(self, cv_image_array, eye_coordinate):
      """ Get the position of eyeball in the rectangular box of the eye
      :param cv_image_array: OpenCV image array from cv2.imread
      :param eye_coordinate: the coordinate of the rectangular box of the eye, from function rect_eye
      :return: percent: the percentage of pixels used to determine the pupil position of the eye among the total pixels
              eyeball_center: the coordinate of the eyeball center
              pupil_position: the position of pupil
      """
      self.eyeball_roi = cv_image_array[eye_coordinate["y1"]:eye_coordinate["y2"], eye_coordinate["x1"]:eye_coordinate["x2"]]
      self.eyeball_roi = cv2.cvtColor(self.eyeball_roi, cv2.COLOR_BGR2GRAY)
      self.gray_val_total = 0
      self.gray_val_min = 255

      for i in range(self.eyeball_roi.shape[0]):  # height
          for j in range(self.eyeball_roi.shape[1]):  # width
              self.gray_val_total += self.eyeball_roi[i][j]
              if self.gray_val_min > self.eyeball_roi[i][j]:
                  self.gray_val_min = self.eyeball_roi[i][j]

      self.gray_val_avg = int(self.gray_val_total / (self.eyeball_roi.shape[0] * self.eyeball_roi.shape[1]))

      self.eyeball_center_x = 0
      self.eyeball_center_y = 0
      self.counter = 0

      if ((self.gray_val_avg * 2) / 3) > self.gray_val_min:
          for i in range(self.eyeball_roi.shape[0]):  # height
              for j in range(self.eyeball_roi.shape[1]):  # width
                  if self.eyeball_roi[i][j] <= ((self.gray_val_avg * 2) / 3):
                      self.eyeball_center_y += i
                      self.eyeball_center_x += j
                      self.counter += 1

      else:
          for i in range(self.eyeball_roi.shape[0]):  # height
              for j in range(self.eyeball_roi.shape[1]):  # width
                  if self.eyeball_roi[i][j] <= self.gray_val_min:
                      self.eyeball_center_y += i
                      self.eyeball_center_x += j
                      self.counter += 1

      self.percent = self.counter / (self.eyeball_roi.shape[0] * self.eyeball_roi.shape[1])

      self.eyeball_center_x = math.ceil(self.eyeball_center_x / self.counter)
      self.eyeball_center_y = math.ceil(self.eyeball_center_y / self.counter)
      self.eyeball_center = {"x": self.eyeball_center_x, "y": self.eyeball_center_y}
      self.pupil_position = self.get_pupil_position(self.eyeball_roi.shape[1], self.eyeball_roi.shape[0], self.eyeball_center)
      return {"percent": self.percent, "eyeball_center": self.eyeball_center, "pupil_position": self.pupil_position}

    def determine_direction(self,left_result, left_percent, right_result, right_percent):
      """ Determine the gaze direction of eyes
      :param left_result: the pupil position of the left eye
      :param left_percent: the percentage of pixels used to determine the pupil position of the left eye among the total pixels
      :param right_result: the pupil position of the right eye
      :param right_percent: the percentage of pixels used to determine the pupil position of the right eye among the total pixels
      :return: determined gaze direction, single result
      """
      if ((left_result == 4) & (right_result != 4)) | ((right_result == 4) & (left_result != 4)):
          return right_result if left_result == 4 else left_result
      elif ((left_result == 1) & ((right_result == 0) | (right_result == 2))) | (
              (right_result == 1) & ((left_result == 0) | (left_result == 2))):
          return right_result if left_result == 1 else left_result
      elif ((left_result == 7) & ((right_result == 6) | (right_result == 8))) | (
              (right_result == 7) & ((left_result == 6) | (left_result == 8))):
          return right_result if left_result == 7 else left_result
      elif ((left_result == 3) & ((right_result == 0) | (right_result == 6))) | (
              (right_result == 3) & ((left_result == 0) | (left_result == 6))):
          return right_result if left_result == 3 else left_result
      elif ((left_result == 5) & ((right_result == 2) | (right_result == 8))) | (
              (right_result == 5) & ((left_result == 2) | (left_result == 8))):
          return right_result if left_result == 7 else left_result
      else:
          return right_result if left_percent < right_percent else left_result


    def interpret_result_direction(self, result):
      """ Interpret the number result into the readable direction
      :param result: the direction result in the numeric code
      :return: the readable direction, like 'right'
      """
      if result == 0 or result == 1 or result == 2:
          return "up"
      elif result == 3:
          return "left"
      elif result == 4:
          return "center"
      elif result == 5:
          return "right"
      elif result == 6 or result == 7:
          return "down"
      else:
          return "closed"

    def PIL2CV(self, img_array):
      return self.pil2cv(img_array)

    def CV2PIL(self, img_array):
      return self.cv2pil(img_array)

    def CV2FR(self, img_array):
      return self.cv2fr(img_array)

    def PIL2FR(self, img_array):
      return self.pil2fr(img_array)

    def FR2PIL(self, img_array):
      return self.fr2pil(img_array)

    def get_gaze_directions(self,cv_img_array,eye_mov_count):
      try:
          result = self.image_preprocessing(cv_img_array)
          if result == None:
            eye_mov_count += 1
            self.get_gaze_direction("/content/",eye_mov_count)
          else:
            #assert result is not None, "No face detected."
            result = self.gaze_direction(result["cv_img_array"], result["eye_locations"])
            self.direction_result = self.determine_direction(result[0], result[1], result[2], result[3])
            return self.interpret_result_direction(self.direction_result)

      except AttributeError as e:
          print(e)
          return "image read error, please check your image path"

      


    def get_gaze_direction(self, path, eye_mov_count):
          em = []
          while eye_mov_count <= 200:
            self.cv_img_array = cv2.imread(path+"f_"+str(eye_mov_count)+".jpg")
            eye_mov_count += 1
            em.append(self.get_gaze_directions(self.cv_img_array,eye_mov_count))
            #print(self.get_gaze_directions(self.cv_img_array,eye_mov_count), eye_mov_count)
          return em
            


    def predictConfidence(self, final_input):
        """
        DESCRIPTION:


        RETURNS:
        
        """
        # -------------------------------- PREDICT --------------------------------

        filename = "/content/drive/MyDrive/Project/Codes/Confidence_Classifier.sav"
        load_model = pickle.load(open(filename, 'rb'))
        res = load_model.predict(final_input)
        # print(res)
        print(type(res))
        res = res.astype("int")
        unq, cnt = np.unique(res, return_counts=True)
        confidence_percentage = (dict(zip(unq, cnt))[1])/( (dict(zip(unq, cnt))[1]) + (dict(zip(unq, cnt))[0]))*100.0
        confidence_level = "NULL"
        if ((confidence_percentage > 0) and (confidence_percentage <=60)):
            confidence_level = "LOW"
        elif ((confidence_percentage>60) and (confidence_percentage <= 80)):
            confidence_level = "MEDIUM"
        elif ((confidence_percentage>80) and (confidence_percentage <=100)):
            confidence_level = "HIGH"

        print("Confidence Percentage: {} \nConfidence Level: {}".format(confidence_percentage, confidence_level))
        confidence=list()
        confidence.append(confidence_percentage)
        confidence.append(confidence_level)

        return confidence
    
    def pre_process(self, expression, direction, eye_mov):
       
      eye_mov = [x for x in eye_mov if x != None]

      expression = np.array(expression)
      direction = np.array(direction)
      eye_mov = np.array(eye_mov)
      print("-------- Before Interpolation-------- \nExpression: {} \nDirection: {} \nEye Movement: {}".format(len(expression), len(direction), len(eye_mov)))
      if (len(expression) < 200):
          unq, cnt = np.unique(expression, return_counts=True)
          value_to_interpolate = unq[np.argmax(cnt)]
          expression = expression.tolist()
          for i in range(200 - len(expression)):
              expression.append(value_to_interpolate)

      if (len(direction) < 200):
          unq, cnt = np.unique(direction, return_counts=True)
          value_to_interpolate = unq[np.argmax(cnt)]
          direction = direction.tolist()
          for i in range(200 - len(direction)):
              direction.append(value_to_interpolate)

      if (len(eye_mov) < 200):
          unq, cnt = np.unique(eye_mov, return_counts=True)
          value_to_interpolate = unq[np.argmax(cnt)]
          eye_mov = eye_mov.tolist()
          for i in range(200 - len(eye_mov)):
              eye_mov.append(value_to_interpolate)

      expression = np.array(expression)
      direction = np.array(direction)
      eye_mov = np.array(eye_mov)
      print("-------- After Interpolation-------- \nExpression: {} \nDirection: {} \nEye Movement: {}".format(len(expression), len(direction), len(eye_mov)))

      # -------------------- Encoding -------------------- 

      direc_encoding_dict = {}
      exp_encoding_dict = {}
      eye_encoding_dict = {}
      direc_encoding_csv = pd.read_csv("/content/drive/MyDrive/Project/Codes/direc_encoding.csv")
      exp_encoding_csv = pd.read_csv("/content/drive/MyDrive/Project/Codes/exp_encoding.csv")
      eye_encoding_csv = pd.read_csv("/content/drive/MyDrive/Project/Codes/eye_encoding.csv")

      for col in direc_encoding_csv.columns:
          direc_encoding_dict[col] = direc_encoding_csv[col].iloc[0]

      for col in exp_encoding_csv.columns:
          exp_encoding_dict[col] = exp_encoding_csv[col].iloc[0]

      for col in eye_encoding_csv.columns:
          eye_encoding_dict[col] = eye_encoding_csv[col].iloc[0]

      for i in range(len(expression)):
          expression[i] = exp_encoding_dict[expression[i]]

      for i in range(len(direction)):
          direction[i] = direc_encoding_dict[direction[i]]

      for i in range(len(eye_mov)):
          eye_mov[i] = eye_encoding_dict[eye_mov[i]]


      # --------------------------- Data Preparation--------------------------- 
      eye_mov = eye_mov.astype("float32")
      direction = direction.astype("float32")
      expression = expression.astype("float32")

      final_input = list()

      for i in range(200):
          temp = list()
          temp.append(expression[i])
          temp.append(direction[i])
          temp.append(eye_mov[i])
          final_input.append(temp)

      final_input = np.array(final_input)
      #print(final_input.shape)
      return final_input

    def generate_report(self,video_name,duration,no_of_face,confidence_lvl):
      pdf = FPDF()
      pdf.add_page()
      pdf.set_font('Arial','B',16)
      pdf.cell(0, 10, 'Confidence Estimation Report', 1, 1, 'C')
      pdf.set_font('Arial','',12)
      pdf.cell(0,8, 'Video Name : '+video_name, ln=1, align='L')
      pdf.cell(0,8, 'Video Duration : '+str(duration), ln=2)
      pdf.cell(0,8, 'Face Detected : '+str(no_of_face),ln=2)
      pdf.cell(0,8, 'Candidate In Video : ',ln=2)
      pdf.image('./f_1.jpg',x=10,y=None,w=70,h=0,type='JPG')
      pdf.set_font('Arial','B',14)
      ch=8
      pdf.ln(ch)
      pdf.cell(0,8, 'Parameter Analysis',border=1,ln=2)
      pdf.cell(0,8, '1. Facial Expression',ln=2)
      pdf.image('./exp_bar_graph.png',x=10,y=None,w=100,h=0,type='PNG')

      pdf.ln(ch)
      pdf.cell(0,8, '2. Face Direction',ln=2)
      pdf.image('./dir_bar_graph.png',x=10,y=None,w=100,h=0,type='PNG')

      pdf.ln(ch)
      pdf.cell(0,8, '3. Eye Movement',ln=2)
      pdf.image('./eyemov_bar_graph.png',x=10,y=None,w=100,h=0,type='PNG')

      pdf.ln(ch)
      pdf.set_font('Arial','B',14)
      pdf.cell(0,8, 'Estimated Confidence Percentage: '+str(confidence_lvl[0]),border=1,ln=2)

      pdf.ln(ch)
      pdf.set_font('Arial','B',14)
      pdf.cell(0,8, 'Estimated Confidence Level: '+str(confidence_lvl[1]),border=1,ln=2)

      pdf.output(f'./confidence_estimation.pdf','F')

    def plot_Expression(self, expression):
      exp_counter = collections.Counter(expression)
      exp = list(exp_counter.keys())
      count = list(exp_counter.values())
      plt.bar(exp, count)
      plt.xlabel("Expressions")
      plt.ylabel("Dominant Expression")
      plt.title("Analysis of FACIAL EXPRESSION Parameter throughout the video")
      plt.savefig('./exp_bar_graph.png', transparent=False, facecolor='white',bbox_inches="tight")

    def plot_Direction(self, direction):
      dir_counter = collections.Counter(direction)
      dir = list(dir_counter.keys())
      count = list(dir_counter.values())
      plt.clf()
      plt.bar(dir, count)
      plt.xlabel("Face Directions")
      plt.ylabel("Dominant Face Direction")
      plt.title("Analysis of FACE DIRECTION Parameter throughout the video")
      plt.savefig('./dir_bar_graph.png', transparent=False, facecolor='white',bbox_inches="tight")

    def plot_Eyemovement(self, eye_mov):
      eye_mov = [x for x in eye_mov if x != None]
      eyemov_counter = collections.Counter(eye_mov)
      eyemov = list(eyemov_counter.keys())
      count = list(eyemov_counter.values())
      plt.clf()
      plt.bar(eyemov, count)
      plt.xlabel("Eye Movements")
      plt.ylabel("Dominant Eye Movement")
      plt.title("Analysis of EYE MOVEMENT Parameter throughout the video")
      plt.savefig('./eyemov_bar_graph.png', transparent=False, facecolor='white',bbox_inches="tight")

      



if (__name__ == "__main__"):

  expression = []
  direction = []
  eye_mov = []
  model_input = []
  confidence_lvl = []
  video_name = 'V_2.mp4'
  
  object = ConfidenceEstimation()
  duration = object.get_video_duration("/content/drive/MyDrive/Project/Codes/V_2.mp4")
  no_of_face = object.VideoToFrame("/content/drive/MyDrive/Project/Codes/V_2.mp4")
  expression = object.getFaceExpressions("/content/")
  print(expression)
  direction = object.getFaceDirection("/content/")
  print(direction)
  eye_mov = object.get_gaze_direction("/content/",eye_mov_count)
  print(eye_mov)
  model_input = object.pre_process(expression, direction, eye_mov)
  #object.load_model()
  confidence_lvl=object.predictConfidence(model_input)
  object.plot_Expression(expression)
  object.plot_Direction(direction)
  object.plot_Eyemovement(eye_mov)
  object.generate_report(video_name,duration,no_of_face,confidence_lvl)

len(expression)

len(direction)

len(eye_mov)